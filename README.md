# RAG App (Retrieval-Augmented Generation)

A simple and modern RAG (Retrieval-Augmented Generation) application built with:
- Python
- Streamlit
- HuggingFace Embeddings
- FAISS Vector Store
- LangChain

This app lets you upload any PDF, convert it into chunks, store embeddings, and ask questions based on the PDF content.


## Features

- Upload any PDF and extract text instantly  
- Automatic chunking of text  
- Embedding generation using HuggingFace  
- FAISS vector store for fast similarity search  
- Ask questions and get answers based ONLY on your PDF  
- Simple UI built with Streamlit  
- Works locally â€” **No API key required**  


ğŸ“¦ Project Structure
rag-app/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ai_sample.pdf
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ loader.py
    â”œâ”€â”€ llm.py
    â”œâ”€â”€ embedding/
    â”‚     â””â”€â”€ embedding.py
    â””â”€â”€ vector_store.py

    
    
ğŸ›  Installation & Running Locally

1ï¸âƒ£ Clone the repository
git clone https://github.com/bhavishraavi/rag-app.git
cd rag-app

2ï¸âƒ£ Create virtual environment
python -m venv .venv
.\.venv\Scripts\activate     # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the app
streamlit run app.py


ğŸ“š How It Works

Upload a PDF

App loads and extracts text

Text is chunked and converted into vector embeddings

FAISS stores the vectors

When you ask a question â†’ FAISS retrieves the best chunks

LLM (HuggingFace) produces an answer based ONLY on that PDF



ğŸ”’ No API Keys Needed

This version works 100% locally using HuggingFace Inference + FAISS.
You donâ€™t need an OpenAI key, HuggingFace token, or internet to run inference.


ğŸ‘¨â€ğŸ’» Author

Bhavish Raavi

GitHub: https://github.com/bhavishraavi

## App Screenshot
![RAG App Screenshot](images/Screenshot_20-11-2025_194552_localhost.jpeg)